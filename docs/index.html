<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Primary Meta Tags -->
  <meta name="title" content="Action-Guided Attention for Video Action Anticipation">
  <meta name="description"
    content="Action-Guided Attention (AGA) explicitly leverages predicted action sequences as queries and keys to guide sequence modeling for video action anticipation.">
  <meta name="keywords"
    content="video action anticipation, attention mechanism, transformer, computer vision, deep learning">
  <meta name="author" content="Tsung-Ming Tai, Sofia Casarin, Andrea Pilzer, Werner Nutt, Oswald Lanz">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Free University of Bozen-Bolzano & NVIDIA">
  <meta property="og:title" content="Action-Guided Attention for Video Action Anticipation">
  <meta property="og:description"
    content="Action-Guided Attention (AGA) explicitly leverages predicted action sequences as queries and keys to guide sequence modeling for video action anticipation.">
  <meta property="og:url" content="https://corcovadoming.github.io/AGA">
  <meta property="og:image" content="https://corcovadoming.github.io/AGA/static/images/backward_analysis.jpg">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Action-Guided Attention for Video Action Anticipation - Research Preview">
  <meta property="article:published_time" content="2026-05-01T00:00:00.000Z">
  <meta property="article:author" content="Tsung-Ming Tai">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Video Action Anticipation">
  <meta property="article:tag" content="Computer Vision">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@TsungMingTai">
  <meta name="twitter:creator" content="@TsungMingTai">
  <meta name="twitter:title" content="Action-Guided Attention for Video Action Anticipation">
  <meta name="twitter:description"
    content="Action-Guided Attention (AGA) explicitly leverages predicted action sequences as queries and keys to guide sequence modeling for video action anticipation.">
  <meta name="twitter:image" content="https://corcovadoming.github.io/AGA/static/images/backward_analysis.jpg">
  <meta name="twitter:image:alt" content="Action-Guided Attention for Video Action Anticipation - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Action-Guided Attention for Video Action Anticipation">
  <meta name="citation_author" content="Tai, Tsung-Ming">
  <meta name="citation_author" content="Casarin, Sofia">
  <meta name="citation_author" content="Pilzer, Andrea">
  <meta name="citation_author" content="Nutt, Werner">
  <meta name="citation_author" content="Lanz, Oswald">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="The Fourteenth International Conference on Learning Representations">
  <meta name="citation_pdf_url" content="https://openreview.net/pdf?id=uKFVZMPppq">

  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">

  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Action-Guided Attention for Video Action Anticipation</title>

  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">

  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- Non-critical CSS - Load asynchronously -->

  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style"
    onload="this.onload=null;this.rel='stylesheet'">

  <!-- Fallback for browsers that don't support preload -->
  <noscript>

    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>

  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">

  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>

  <script defer src="static/js/index.js"></script>

  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Action-Guided Attention for Video Action Anticipation",
    "description": "Action-Guided Attention (AGA) explicitly leverages predicted action sequences as queries and keys to guide sequence modeling for video action anticipation.",
    "author": [
      {
        "@type": "Person",
        "name": "Tsung-Ming Tai",
        "affiliation": {
          "@type": "Organization",
          "name": "Free University of Bozen-Bolzano & NVIDIA"
        }
      },
      {
        "@type": "Person",
        "name": "Sofia Casarin",
        "affiliation": {
          "@type": "Organization",
          "name": "Free University of Bozen-Bolzano"
        }
      },
      {
        "@type": "Person",
        "name": "Andrea Pilzer",
        "affiliation": {
          "@type": "Organization",
          "name": "NVIDIA"
        }
      },
      {
        "@type": "Person",
        "name": "Werner Nutt",
        "affiliation": {
          "@type": "Organization",
          "name": "Free University of Bozen-Bolzano"
        }
      },
      {
        "@type": "Person",
        "name": "Oswald Lanz",
        "affiliation": {
          "@type": "Organization",
          "name": "Free University of Bozen-Bolzano"
        }
      }
    ],
    "datePublished": "2026-05-01",
    "publisher": {
      "@type": "Organization",
      "name": "ICLR 2026"
    },
    "url": "https://corcovadoming.github.io/AGA",
    "image": "https://corcovadoming.github.io/AGA/static/images/backward_analysis.jpg",
    "keywords": ["video action anticipation", "attention mechanism", "transformer", "computer vision", "deep learning"],
    "abstract": "Anticipating future actions in videos is challenging, as the observed frames provide only evidence of past activities, requiring the inference of latent intentions to predict upcoming actions. Existing transformer-based approaches, which rely on dot-product attention over pixel representations, often lack the high-level semantics necessary to model video sequences for effective action anticipation. As a result, these methods tend to overfit to explicit visual cues present in the past frames, limiting their ability to capture underlying intentions and degrading generalization to unseen samples. To address this, we propose Action-Guided Attention (AGA), an attention mechanism that explicitly leverages predicted action sequences as queries and keys to guide sequence modeling. Our approach fosters the attention module to emphasize relevant moments from the past based on the upcoming activity and combine this information with the current frame embedding via a dedicated gating function. The design of AGA enables post-training analysis of the knowledge discovered from the training set. 
Experiments on the widely adopted EPIC-Kitchens-100 benchmark demonstrate that AGA generalizes well from validation to unseen test sets. Post-training analysis can further examine the action dependencies captured by the model and the counterfactual evidence it has internalized, offering transparent and interpretable insights into its anticipative predictions.",
    "citation": "@inproceedings{tai2026action,\n  title={Action-Guided Attention for Video Action Anticipation},\n  author={Tai, Tsung-Ming and Casarin, Sofia and Pilzer, Andrea and Nutt, Werner and Lanz, Oswald},\n  booktitle={The Fourteenth International Conference on Learning Representations},\n  year={2026},\n  url={https://openreview.net/forum?id=uKFVZMPppq}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://corcovadoming.github.io/AGA"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Video Action Anticipation"
      },
      {
        "@type": "Thing", 
        "name": "Computer Vision"
      }
    ]
  }
  </script>

  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Free University of Bozen-Bolzano & NVIDIA",
    "url": "https://www.unibz.it/",
    "logo": "https://corcovadoming.github.io/AGA/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/TsungMingTai",
      "https://github.com/CorcovadoMing"
    ]
  }
  </script>
</head>

<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">Action-Guided Attention for Video Action Anticipation</h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="" target="_blank">Tsung-Ming Tai</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Sofia Casarin</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Andrea Pilzer</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Werner Nutt</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="" target="_blank">Oswald Lanz</a><sup>1</sup></span>
              </div>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup>1</sup>Free University of Bozen-Bolzano, <sup>2</sup>NVIDIA<br>ICLR
                  2026</span>
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://openreview.net/forum?id=uKFVZMPppq&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2026%2FConference%2FAuthors%23your-submissions)"
                      target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/CorcovadoMing/AGA" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- TODO: Update with your arXiv paper ID -->
                  <span class="link-block">
                    <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>


    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img src="static/images/backward_analysis.jpg" alt="First research result visualization" loading="lazy" />
          <h2 class="subtitle has-text-centered">
            <!-- The model consists of two modules. The Action-Guided Attention uses the most recent $S$ action predictions
            as keys, the exponential moving average (EMA) of all predicted actions as the query, and $S$ frame
            embeddings as values to generate a history context $\tilde{h}_t$. The \textit{Adaptive Gating} then
            integrates this history context with the current frame embedding $e_t$ to produce a fused representation,
            which is mapped to the new prediction $\hat{y}_t$. -->
          </h2>
        </div>
      </div>
    </section>

    <!-- Paper abstract -->
    <section class="section hero is-light">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Anticipating future actions in videos is challenging, as the observed frames provide only evidence of
                past activities, requiring the inference of latent intentions to predict upcoming actions. Existing
                transformer-based approaches, which rely on dot-product attention over pixel representations, often lack
                the high-level semantics necessary to model video sequences for effective action anticipation. As a
                result, these methods tend to overfit to explicit visual cues present in the past frames, limiting their
                ability to capture underlying intentions and degrading generalization to unseen samples. To address
                this, we propose Action-Guided Attention (AGA), an attention mechanism that explicitly leverages
                predicted action sequences as queries and keys to guide sequence modeling. Our approach fosters the
                attention module to emphasize relevant moments from the past based on the upcoming activity and combine
                this information with the current frame embedding via a dedicated gating function. The design of AGA
                enables post-training analysis of the knowledge discovered from the training set.
                Experiments on the widely adopted EPIC-Kitchens-100 benchmark demonstrate that AGA generalizes well from
                validation to unseen test sets. Post-training analysis can further examine the action dependencies
                captured by the model and the counterfactual evidence it has internalized, offering transparent and
                interpretable insights into its anticipative predictions.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- End paper abstract -->


    <!--BibTex citation -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <div class="bibtex-header">
          <h2 class="title">BibTeX</h2>
          <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
            <i class="fas fa-copy"></i>
            <span class="copy-text">Copy</span>
          </button>
        </div>
        <pre id="bibtex-code"><code>@inproceedings{tai2026action,
  title={Action-Guided Attention for Video Action Anticipation},
  author={Tai, Tsung-Ming and Casarin, Sofia and Pilzer, Andrea and Nutt, Werner and Lanz, Oswald},
  booktitle={The Fourteenth International Conference on Learning Representations},
  year={2026},
  url={https://openreview.net/forum?id=uKFVZMPppq}
}</code></pre>
      </div>
    </section>
    <!--End BibTex citation -->


    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">

              <p>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a> which was adopted from the <a
                  href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                You are free to borrow the source code of this website, we just ask that you link back to this page in
                the footer. <br> This website is licensed under a <a rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                  Commons Attribution-ShareAlike 4.0 International License</a>.
              </p>

            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>

</html>